# Mesterséges neuronhálók

**Mesterséges neuron**: 
- bemenő értékekből kimenő értéket számoló egység
- számítási képlete változtatható, tanítható

**Hálózati topológia**: 
- sok mesterséges neuron egymáshoz kapcsolva, ahol egyik neuron kimenete egy másik neuron bemenete lesz
- egyesek bemenetei hálózaton kívülről is jöhet, illetve kimenetei a hálózat kimenetének tekinthető

**Tanulási szabály**:
- neuron számítási képletét meghatározó eljárás
- lehet tanító példák alapján működő algo.

**Perceptron**:
- **Bemenet**: $x_{0} \neq 0$
- **Kimeneti függvény**: $g(I) = g\left( \sum^n_{i=1} w_{i} * x_{i} + w_{0} * x_{0} \right)$
	- $b := w_{0} * x_{0}$
- **Összegzett bemenet**: $I = \sum^n_{i=0}w_{i}*x_{i}= \underline{w}^T*\underline{x}$

## Kimeneti függvények
- $$step(x):= \begin{cases}
  0, &x \leq 0\\ \\
1, & x > 0

\end{cases}$$
- $$sgn(x) = \begin{cases}
1, & x \geq 0 \\
-1, & x < 0
\end{cases}$$
- $$linear_{a,b}(x) = \begin{cases}
0, & x < a \\
1 -\frac{b - x}{b-a}, & a \leq x \leq b \\
1, & x > b
\end{cases}$$ Lineáris interpoláció $a$ és $b$ között
- $$relu(x) = max(0,x)$$ 
- $$sigmoid(x) = \frac{1}{1+e^{-x}}$$
-  $$\tanh(x) = \frac{1-e^{-x}}{1+e^{-x}}$$
- $$\sin(x)$$
- $$softmax(x) = \frac{e^{x_{j}}}{\sum e^{x_{k}}}$$ 

## Hálózati topológia
**Irányított gráf**
- **Csúcsok**: mesterséges  neuronok, rétegekbe csoportosíthatók
- **Irányított élek**: adatáramlás irányát jelölik ($a \to b$) 

**Speciálisan**: 
- visszacsatolás ~ vissza él
- rétegen belüli kapcsolat ~ hurok él
- rétegen átívelő kapcsolat
- speciális, bemenő értékeket jelölő csúcsok
- szomszédos rétegek közötti kapcsolatok

## Konvolúciós neuron hálózat
Tárolt súlyok mennyiségének csökkentése miatt hasznos

## Rekurrens neurális hálózat
- Input és/vagy output változó hosszúságú sorozat
- Számítási képlet az előző inputokra való emlékezést utánozza

## Általánosított perceptron tanulása
- Neuron számítási képletét a neuron $w$ súlyai határozzák meg.
- Súlyok implicit módon a hálózat topológiáját is kijelölik
- **Tanulás során** súlyok fokozatosan módosulnak ($w:=w+\Delta w$)
- $\Delta w$ a neuron **bemeneti értékeitől** és a neuron által **kiszámított kimeneti értéktől** függ
	- **Felügyelt tanulás**: felhasználjuk az elvárt kimenetet
	- **Felügyelet nélküli tanulás**: elvárt kimenetre nincs szükség

## Lineáris szeparálhatóság
Egyetlen perceptronnal megoldható a feladat, ha: bemeneteket egy hipersík választja szét (*lineárisan szeparálhatók*)

- Több réteggel összetettebb problémák is megoldhatók
- DE: $step$ nem deriválható -> nem sikerült megfelelő tanuló algo.-t találni ehhez
- Másik kimeneti fv.-el viszont már létrehozható ilyen algo.!

## Homogén MLP háló számítási modellje
- Teljes háló számítási modellje: $f(\theta,x)= g(w^{[r]}*\dots*g(w^{[s]}*\dots*g(w^{[2]}*g(w^{[1]}*x))))$
- Réteg számítási modellje: $o^{[s]} = g(w_{j}^{[s]}*o^{[s-1]})= g\left( \sum^{n^{[s-1]}}_{i =0} w_{ij}^{[s]}*o_{i}^{[s-1]}  \right)$

## Hiba visszaterjedés módszere (error backpropagation)

**Modell hibafv.**: $L(\theta) = \frac{1}{2} *\sum^{n^{[r]}}_{j=1}(y_{j}-o_{j}^{[r]})^2$
Tanulás során ennek a fv.-nek keressük a minimum helyét *gradiens módszerrel* (lépésről lépésre módosítjuk a súlyokat megfelelő irányban kis mértékben) 

### Algo.
1. $\underline{x}$ bemeneti vektorból indulva rétegenként számoljuk a neuronok kimenetét: $o_{j}^{[s]}$, a kimeneti réteg kimeneteit $o_{j}^{[r]}$ is.
2. Kimeneti réteg minden nueronjára kiszámoljuk a lokális hibát.
3. Rétegenként hátulról előre haladva számoljuk a belső neuronok hibáit
4. Végül módosítjuk a hálózat súlyait.

## Rétegenként eltérő aktivizációs függvény
- Gradiens elméleti kiszámítása nehéz => numerikus módszerek használata (TensorFlow, Keras)
- További problémák:
	- Tanító minták kiválasztása nehéz
	- Hiper-paraméterek megtanulásának kérdése
	- Lokális minimum, illetve nyeregpontok problémája

## Hopfield modell
Aszinkront működésű Hopfield topológia:
- egyrétegű
- teljes összekötöttségű hálózat $n$ darab +1 vagy -1 állapotú neuronnal

### Működése
**Kezdet**: neuronok kívülről kapják értékül az állapotaikat. Neuronok által felvett állapot-együttest hívjuk a háló egy *konfigurációjának*.

**Ezután**: Állapotok újra számolása *aszinkron módon*, többször is, folyamatosan változtatva a hálózat konfigurációját

**Végül**: Amikor a hálózat egy stabil konfigurációba jut, akkor a neuronok állapotait a háló kimeneteinek tekintjük.

### Felhasználása
- Eredetileg *asszociatív memória* megvalósítására készült => hálózat 1-1 stabilt konfigja ad meg egy eltárolt mintát
- Amikor a minta zajos/torzított/hiányos => hálózat addig változtatgatja az állapotát a neuronjainak, amíg újra stabil konfigurációba nem kerül, azaz nem idézi fel az eredeti mintát

### Egyetlen minta tárolása

Minden olyan konfig., amely komponenseinek több, mint a fele azonos a mintáéval, a háló véges lépésben a mintához konvergál.

### Több minta tárolása

Mivel a konfigurációk száma véges, így a hálózat véges lépésben stabil konfigurációba fog jutni