# Gépi tanulás

**Tanulás**: Változások működés közben, amellyel később ugyanazon/hasonló a feladatot jobb eredménnyel/hatékonysággal képes megoldani, mint korábban.

**Tanulással meg lehet adni a feladat**: 
- modelljét
- megoldó algoritmusát
- heurisztikáját

**Megoldandó probléma**: 
- modellje: $\phi: X \to Y$ 
- approximációja: $f:X\to Y$
- **azaz**: $\phi \approx f$
**Sokszor azonban . . .**
- rögzített $f:P\times X\to Y$ leképzés
- $\theta\in P$ paraméter keresése, amelyre $f(\theta,x) \approx \phi(x)$ teljesül

## Induktív tanulási modell
$f$ leképzést $x_{n}\in X,(n=1\dots N)$ bemenetek (*minták*) alapján tanuljuk
### Módjai
**Felügyelt tanulás**: 
- Ismeri a tanuláshoz használt minták elvárt kimenetét is 
- Input-output párok használata

**Felügyelet nélküli tanulás**: 
- Nem ismeri a minták elvárt kimenetét
- Minták, ill. azokra kiszámolt kimenetek közötti összefüggéseket próbálja *felismerni*/*osztályozni*

**Megerősítéses tanulás**:
- Nem ismeri a minták elvárt kimenetét
- Képes az $x_{n}(n = 1\dots N)$ inputokra kiszámolt eredményt minősíteni (*mennyire megfelelő?*)

### Felügyelt tanulás

#### Paraméteres tanulás
$\phi:X\to Y$ közelítése $f:P \times X\to Y$ paraméteres leképzéssel.
$\theta\in P$ keresése (*paraméteres tanulás*), **úgy hogy**:
	$L(\theta) = \frac{1}{N}* \sum_{n=1}^Nl(f(\theta,x_{n}),y_{n})$ legyen elég kicsi.
	**Ahol**:
- $f(\theta,x_{n}) = t_{n}$ ~ számított kimenet
- $y_{n}$ ~ elvárt kimenet
- $l:Y\times Y\to \mathbb{R}$ *hibafüggvény*

**Megfontolások**:
- $f(\theta,x)$ kiszámítása gyors legyen
- $\theta$ megtanulása akkor működik jól, ha:
	- $N$ elég nagy
	- $f$ és $l$ megfelelőek
	- $\theta$ közel esik a paraméter globális optimumához
- $\theta$ megtalálása egy *nem-konvex optimalizálási feladat* (NP-teljes probléma)
- Ez viszont nem is cél, mert *túltanuláshoz* vezetne
#### K legközelebbi szomszéd
$x\in X$ bemenethez annak a $K$ darab mintának az outputja alapján számol kimenetet, amely minták inputja a legközelebb esik az $x$-hez

$sort(x,P,K)$: $P = \{ x_{n}|n =1\dots N \}$ tanító minták bemeneteiből képzett, az $x$-től vett távolság alapján növekvő sorozat első $K$ eleme.
- $f(\theta,x) = \sum_{n=1..N} \frac{I(x_{n}\in sort(x,P,K))}{K}*y_{n}$
- $f(\theta,x) = arg\  \max_{n=1\dots N}\sum_{\phi(x_{i}) =y_{n}}x_{i} \in sort(x,P,K)^1$
- $\theta$ paraméter a minták és a $K\in \mathbb{N}$ szám együttese
- legközelebbi szomszédokat az $\lvert \lvert x_{n} -x \rvert \rvert^2_{2}$ távolságok sorba rendezésével választjuk ki

**Előny**: egyszerű programozni, tanulás gyors
**Hátrány**: ha $N$ nagy, a tárolás költséges; $f$ kiszámítása (minták sorbarendezése) költséges

#### Döntési fa

T.f.h. $x\in X$ bemeneteknek ugyanazon tulajd.-it ismerjük, egy bemenetet *attribútum-érték párok halmazával jellemezhetünk*

**Irányított fa**, amely:
- **Belső csúcsai**: 1-1 attribútum, a kivezető ágak ezen attribútumok lehetséges értékeit címzik
- **Ágai**: attribútum-érték párok halmazát jelölik ki
- **Levelei**: 1-1 lehetséges kimeneti érték

Egyértelműen leképezhető a döntési fa egyik levelére az $x$ bemenet az attribútum-érték párjai alapján.

Ugyanazon problémához több döntési fa is!

##### Építése
Minták segítségével.
1. Minden csúcshoz hozzárendeljük azon tanító mintákat, amelyek a csúcshoz vezető ág attribútum-érték párjaival rendelkeznek
2. **Belső csúccsá válás**: egy - hozzávezető ágon nem szereplő - attribútummal címkézzük fel.
3. **Levélcsúccsá nyilvánítás**: ha nincsenek tanító mintái **vagy** mind hasonló kimenetű **vagy** a csúcshoz vezető ágon már minden attr. szerepel
4. **Levélcsúcs értéke**: hozzátartozó minták kimeneteinek átlaga **vagy** leggyakoribb kimenete
	- Ha nem tartoznak minták, vagy nem egyértelmű a leggyakoribb kimenet, akkor a szülőcsúcsnak mintái alapján számolunk.

##### Heurisztika
Döntési fa minél kisebb, annál rövidebbek az ágai, annál jobb!
Ehhez **szükséges**: egy csúcs tanító mintáinak kimenetei között kicsi legyen az eltérés **vagy** minél kevésbé legyenek a kimenetek változatosak (*entrópia*)

##### Entrópia / Információ tartalom
$P$-beli minták információtartalma:
- $E(P) = E(p_{1},\dots p_{n})= -\sum^n_{i=1}p_{i}\log_{2}p_{i}$
- $P$-ben $n$-féle eltérő értékkel rendelkező minta van, amelyek $P$-beli gyakoriságának aránya $p_{1}:\dots:p_{n}$ és $p_{1}+\dots+p_{n}=1$

##### Információs előny számítása
$C(P,a) = E(P) - \sum_{v\inÉrték(a)} \frac{\lvert P_{a=v} \rvert}{\lvert P \rvert}*E(P_{a=v})$
- $P$: szülő csúcs mintái
- $a$: választott attr.
- $Érték(a)$: $a$ attr. által felvett értékek
- $P_{a=v}=\{ p\in P |p.a=v \}$

Ezek alapján információs előnyre tehetünk szert a megfelelő attribútum kiválasztásával.

##### Algo.
1. Fokozatosan épülő döntési fában minden csúcsnál eltároljuk:
	- csúcshoz tartozó **tanító mintákat**
	- csúcsnál még választható **attribútumokat**
2. Egy csúcs lehet
	- Attribútummal *címkézett belső csúcs*, amelyből kivezető éleket az attribútum lehetséges értékei címkézik
	- *Kiértékelt vagy értékkel nem rendelkező levélcsúcsok*
3. Minden lépésben egy értékkel még nem rendelkező levélcsúcsról kell eldönteni, hogy kaphat-e értéket vagy belső csúcs legyen

**Kezdetben**:
- 1 címkézetlen csúcs az összes mintával és attr.-al
**Lépés**: veszünk a fából egy értékkel nem rendelkező levélcsúcsot, amíg van ilyen. **Ha**:
1. Csúcsnak nincsenek mintái => szülőcsúcsának mintái alapján kiszámoljuk a csúcs értékét
2. Csúcshoz tartozó minták kimenete **nem nagyon** tér el egymástól => kiszámoljuk a csúcs értékét
3. Csúcsnak nincsenek választható attribútumai ($A = \emptyset$) => csúcs mintái alapján kiszámoljuk a csúcs értékét
4. **Egyébként**: választható attr.-ok közül a legkedvezőbbel megcímkézzük a csúcsot, gyerekeit legeneráljuk a hozzátartozó mintákkal és választható attr.-okkal

##### Megjegyzés
**Zaj**: azonos attr.-értékű minták kimenete különbözik => minták válaszainak átlagolása félrevezethet

**Túlzott illeszkedés**: Kimenetre 0 hatással lévő attr.-ok figyelembe vétele => lényegetelen attr-okat állítsuk félre ($C(P,a)$ ~ 0)

**Általánosítások**: 
- hiányzó adatok problémája (attr. értékek)
- folytonos értékű attr.

##### Tanulás
Döntési fában az $x\in X$ bemenetre kiszámolt levélcsúcs értéke:
- $f(\theta,X)=\sum_{n=1\dots N} \frac{I(x_{n}\in P(x))}{\lvert P(x) \rvert}*y_{n}$
- $f(\theta,X) = arg\ \max_{n=1\dots N}\sum_{\phi(x_{i})=y_{n}}x_{i}\in P(x)^1$
- $\theta$ a döntési fa, amely optimalizálása annak mohó felépítése

##### Értékelés
**Előny**: 
- jól értelmezhető
- tanító minták helyett csak a döntési fát kell tárolni
- $x$-re adott eredmény gyorsan számolható

**Hátrány**:
- optimális döntési fa építése NP-teljes
- bemutatott módszerrel csak lokálisan optimális döntési fához jutunk

#### Tanulás véletlen erdővel
$K$ darab döntési fát építünk a tanító minták alapján. Egy fa építéséhez:
- tanító minták $P_{k} \subseteq P$
- attribútumok $A_{k} \subseteq A$ 
- $\dots$ csak egy-egy véletlen kiválasztott részhalmazát használjuk fel.
Ez a **véletlen erdő**.

##### Tanulás
$x$ bemenethez tartozó kimenetet a minták kimeneteinek súlyozott átlaga, ahol a súlyok attól függenek, hogy egy minta a véletlen erdő döntési fáinak $x$-re kiszámolt levélcsúcsaihoz tartozó mintahalmazok közül hányba esik bele, és az a halmaz hány elemű

- $f(\theta,x) = \sum^K_{k=1} \frac{value(x,decision\_tree(A_{k},P_{k}))}{K}$
- $\theta$ maga a véletlen erdő, optimalizálása az erdő felépítése

##### Értékelés

**Előny**: 
- tanító minták helyett az erdő tárolása
- véletlen generálás miatt kevésbé mohó, elkerüli a túltanulást
- $x$-re adott eredmény számolása párhuzamosítható

**Hátrány**:
- Eredmény kevésbé értelmezhető
- Erdő-építés NP-teljes

### Felügyelet nélküli tanulás
**Például**:
- Klaszterezés
- Dimenzió csökkentés
- Autóenkóderek

#### k-közép módszer
**Legyen**:
- $x_{p}$ klaszterezendő elemek $\mathbb{R}^n$-beli pontok, soroljuk be ezeket $k$ darab klaszter valamelyikébe

**Klaszter ($S_{i}$) repr.:** középpontjával ($m_{i}$), ami a klaszterhez tartozó pontok átlaga (*centroid*)
- $m_{i}= \frac{1}{\lvert S_{i} \rvert}*\sum_{x_{j\in S_{i}}} x_{j}$

**Keressük**: azt a $k$ klasztert, amelyre minimális az elemeknek a klaszterük középpontjától számított távolságnégyzeteinek összege
$\min_{S} \sum^k_{i=1} \frac{1}{2*\lvert S_{i} \rvert}*\sum_{x,y\in S_{i}}\lvert \lvert x-y \rvert \rvert^2_{2}$


##### Algo.
**Kezdet**: adott $k$ és $\mathbb{R}^n$-beli $x_{p}$ elemek
1. Inicializáljuk a $k$ darab centroidot (véletlenszerűen $k$ középpont választása **vagy** minden adatpontot véletlenszerűen 1-1 klaszterbe sorolunk, majd középpontjait kiszámoljuk)

**Lépések**: ($t$-edik változatról $t+1$-dikre)
1. Elemeket a legközelebbi klaszter középponthoz rendeljük:  $S_{i}^{t+1} = \{ x_{p}| \forall j \in [1\dots k]: \lvert \lvert x_{p} - m_{i}^{(t)} \rvert \rvert^2_{2} \leq \lvert \lvert x_{p} - m_{j}^{(t)} \rvert \rvert^2_{2} \}$
2. Kiszámítjuk az új elemcsoportok középpontjait: $m_{i}^{(t+1)} = \frac{1}{\lvert S_{i}^{t+1} \rvert}*\sum_{x_{j}\in S_{j}^{t+1}}x_{j}$

**Terminálás**: ha két lépés eredménye közötti eltérés adott határ alá kerül

#### Megjegyzés
- [[#k-közép módszer]] ~ kemény klaszterező eljárás (*hard clustering*)
- Klaszterek megtanulása után bármelyik $\mathbb{R}^n$-beli pont besorolható a meglévő klaszterek közül a legközelebbibe.
- Tulajdonképpen osztályozza a $X$-beli elemeket
- Optimális klaszterek megtalálása *NP-nehéz* => approximáció alkalmazása (viszont csak lokális optimumot biztosít)
- **Probléma, ha** $k$: 
	- *túl nagy*: kevés elemeket tartalmazó "üres" klaszterek kialakulása
	- *túl kicsi*: klaszteren belül szignifikánsan elkülönülő csomódások kialakulása
- Többször futtatható különböző véletlen inicializációval
## Adaptív (inkrementális) tanulás
Egy már megtanult $f$ leképzést egy új minta anélkül módosít, hogy a korábbi mintákat újra meg kell vizsgálnunk

